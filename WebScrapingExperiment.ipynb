{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# books-to-scrape-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_books_from_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Memeriksa apakah permintaan berhasil\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Mengambil semua buku\n",
    "    books = soup.find_all('article', class_='product_pod')\n",
    "    book_data = []\n",
    "\n",
    "    for book in books:\n",
    "        # Mengambil judul buku\n",
    "        title = book.h3.a['title']\n",
    "\n",
    "        # Mengambil harga buku\n",
    "        price = book.find('p', class_='price_color').text\n",
    "\n",
    "        # Mengambil ketersediaan buku\n",
    "        availability = book.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "        # Mengambil rating buku\n",
    "        rating_class = book.p['class']\n",
    "        rating = rating_class[1] if len(rating_class) > 1 else \"No rating\"\n",
    "\n",
    "        # Mengambil URL gambar sampul\n",
    "        image_url = book.find('img')['src']\n",
    "        image_url = 'https://books.toscrape.com/' + image_url.replace('../', '')\n",
    "\n",
    "        # Menyimpan data buku dalam bentuk dictionary\n",
    "        book_data.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Availability': availability,\n",
    "            'Rating': rating,\n",
    "            'Image URL': image_url\n",
    "        })\n",
    "\n",
    "    return book_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Scrape Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_pages(base_url, total_pages):\n",
    "    all_books = []\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        if page == 1:\n",
    "            url = base_url  # Halaman pertama\n",
    "        else:\n",
    "            url = f\"{base_url}catalogue/page-{page}.html\"  # Halaman berikutnya\n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "        books = scrape_books_from_page(url)\n",
    "        if books:\n",
    "            all_books.extend(books)\n",
    "        time.sleep(1)  # Memberikan jeda untuk menghindari terlalu banyak request\n",
    "\n",
    "    return all_books\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Save Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    if not data:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    print(f\"Data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Scrapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentukan URL dan jumlah halaman yang ingin di-scrape\n",
    "base_url = 'https://books.toscrape.com/'\n",
    "total_pages = 5  # Ubah sesuai jumlah halaman yang diinginkan\n",
    "\n",
    "# Lakukan scraping dan simpan data\n",
    "books_data = scrape_multiple_pages(base_url, total_pages)\n",
    "save_to_csv(books_data, 'books_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show data on Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca file CSV dan menampilkan beberapa baris pertama\n",
    "df = pd.read_csv('books_data.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
